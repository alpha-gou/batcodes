### model
model_name_or_path: /workspace/models/Qwen2.5-72B-Instruct

### method
stage: pt
do_train: true
finetuning_type: full
deepspeed: ds_z3_config.json

### dataset
dataset: train_chin_all_18hq_38w_v0_mix,train_chin_all_18mq_57w_v0_mix,train_chin_all_xkw_pic_133w_mix,train_chin_all_2hq_119w_v0_mix,train_chin_all_2hq_119w_v0_mix,train_chin_all_51lq_1875w_v1_mix,train_chin_all_jtkhq_3w_v0_mix,train_chin_all_spider_551w_v1_mix,train_chin_all_spider_pic_352_v1_mix,train_chin_all_spider_pic_83w_v1_mix,train_chin_all_xkw_125w_v0_mix
# tokenized_path: 
template: qwen
cutoff_len: 2048
# max_samples: 160000
overwrite_cache: true
preprocessing_num_workers: 32

### output
output_dir: output
logging_steps: 1
save_steps: 1000
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 6
gradient_accumulation_steps: 1
learning_rate: 3.0e-5
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### eval
val_size: 0.001
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 1000

# save mode
save_only_model: true

### report to wandb
report_to: wandb
# run_name: Qwen2.5-72B-Instruct-lf_chin_all_clm_3455w_v1_3 
